---
title: "Datasets Overview"
icon: "database"
description: "Explore the types, formats, and quality of datasets on Lomen Hub."
---

Lomen Hub provides access to a growing range of blockchain datasets specifically curated and structured for AI applications and autonomous agents.

## Types of Datasets

Our goal is to offer diverse datasets covering various aspects of blockchain activity. Examples include:

- **Market Trends:** Real-time and historical data on token prices, trading volumes, and market sentiment across different exchanges and chains.
- **DeFi Metrics:** Comprehensive data on lending protocols, liquidity pools, staking rewards, and other decentralized finance activities.
- **Token Flows:** Analysis of token movements between wallets, smart contracts, and exchanges.
- **NFT Activity:** Data related to NFT collections, minting events, sales history, and ownership changes.
- **Prediction Market Outcomes:** Datasets relevant for training agents to participate in or analyze prediction markets.
- **Transaction Analytics:** Detailed information extracted from blockchain transactions, potentially labeled or categorized for specific use cases like sybil detection.

## AI-Ready Formats

Raw blockchain data is often difficult for AI models to consume directly. Lomen Hub transforms data into user-friendly and AI-optimized formats:

- **Structured Data:** Tabular formats (like CSV), time-series data suitable for analysis and model training.
- **Queryable Formats:** Accessible via familiar interfaces like SQL queries or API responses in formats like JSON.
- **RAG-Friendly:** Datasets designed for seamless integration with Retrieval-Augmented Generation (RAG) pipelines used by Large Language Models (LLMs), enabling them to query and reason about live blockchain data.
- **Pre-processed:** Data undergoes necessary cleaning, parsing, and indexing to be immediately usable.

## Quality and Reliability

We understand that the reliability of AI depends heavily on the quality of the underlying data. Lomen Hub emphasizes:

- **Verification:** Implementing protocols to ensure data accuracy and integrity.
- **Reputation:** Utilizing a provider reputation system to highlight trusted and high-quality data sources.
- **Consistency:** Ensuring data is consistently structured and maintained across different datasets and chains.
